{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as sci\n",
    "import sys, os, warnings, random, datetime\n",
    "from sys import platform\n",
    "\n",
    "#\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Gradeint decent models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED   = \"\\033[1;31m\"  \n",
    "BLUE  = \"\\033[1;34m\"\n",
    "RESET = \"\\033[0;0m\"\n",
    "BOLD    = \"\\033[;1m\"\n",
    "REVERSE = \"\\033[;7m\"\n",
    "\n",
    "\n",
    "def gen_data():\n",
    "    #setting path  \n",
    "    if platform == \"win32\" and os.getlogin() == 'zaine':\n",
    "        data_path = 'C:/Users/zaine/OneDrive/Desktop/School/STAT/Reg_Proj/ieee-fraud-detection/'\n",
    "        ScaleData = 0\n",
    "    elif platform == \"win32\" and os.getlogin() == 'Avery':\n",
    "        data_path = \"C:/Users/Avery/Desktop/Applied_Regression/Datasets for reg/\"\n",
    "        ScaleData = 1\n",
    "    elif platform == \"darwin\":\n",
    "        data_path = \"/Users/zain/Desktop/School/Reg_Proj/ieee-fraud-detection/\"\n",
    "        ScaleData = 2\n",
    "        \n",
    "\n",
    "    #a bunch of logic related to what system we are on\n",
    "    if(ScaleData == 0):\n",
    "        train_ident = pd.read_csv(data_path + 'train_identity.csv')\n",
    "        test_ident = pd.read_csv(data_path + 'test_identity.csv')\n",
    "        train_transaction = pd.read_csv(data_path + 'train_transaction.csv')\n",
    "        test_transaction = pd.read_csv(data_path + 'test_transaction.csv')\n",
    "        #Merging our 4 data sets into 2:\n",
    "        train = train_transaction.merge(train_ident, on='TransactionID', how='left')\n",
    "        test = test_transaction.merge(test_ident, on='TransactionID', how='left')\n",
    "        \n",
    "        cur_working = os.path.dirname(os.getcwd()) \n",
    "        fptrain = str(cur_working) + \"\\dsProject\\\\train_sample.csv\"\n",
    "        fptest = str(cur_working) + \"\\dsProject\\\\test_sample.csv\"\n",
    "        train_sample = pd.read_csv(fptrain)\n",
    "        test_sample = pd.read_csv(fptest)\n",
    "    elif(ScaleData == 1):\n",
    "        cur_working = os.path.dirname(os.getcwd()) \n",
    "        train = pd.read_csv(data_path + \"/train_sample.csv\") #make this exact path to the csv\n",
    "        test = pd.read_csv(data_path + \"/test_sample.csv\") #make this exact path to the csv\n",
    "        cur_working = os.path.dirname(os.getcwd()) \n",
    "        fptrain = str(cur_working) + \"\\dsProject\\\\train_sample.csv\"\n",
    "        fptest = str(cur_working) + \"\\dsProject\\\\test_sample.csv\"\n",
    "        train_sample = pd.read_csv(fptrain)\n",
    "        test_sample = pd.read_csv(fptest)\n",
    "    elif(ScaleData == 2):\n",
    "        train = pd.read_csv(data_path + \"/train_sample.csv\")\n",
    "        test = pd.read_csv(data_path + \"/test_sample.csv\")\n",
    "        train_sample = pd.read_csv(data_path + \"/train_sample.csv\")\n",
    "        test_sample = pd.read_csv(data_path + \"/train_sample.csv\")\n",
    "    else:\n",
    "        sys.stdout.write(RED)\n",
    "        error_code= input(\"Error: Data path not found do you want to regenearte the train and test sets in the current working directory?[y/n]: \")\n",
    "        if error_code == 'y':\n",
    "            print(\"Warning: Data path not found regenerating data in current script working directory!\")\n",
    "            sys.stdout.write(RESET)\n",
    "            print(\"\\n==============================================================\")\n",
    "            train_ident = pd.read_csv(data_path + 'train_identity.csv')\n",
    "            test_ident = pd.read_csv(data_path + 'test_identity.csv')\n",
    "            train_transaction = pd.read_csv(data_path + 'train_transaction.csv')\n",
    "            test_transaction = pd.read_csv(data_path + 'test_transaction.csv')\n",
    "            #Merging our 4 data sets into 2:\n",
    "            print(\"Merging Data\")\n",
    "            train = train_transaction.merge(train_ident, on='TransactionID', how='left')\n",
    "            test = test_transaction.merge(test_ident, on='TransactionID', how='left')\n",
    "            print(\"Sampling Data\")\n",
    "            train_sample = train.sample(frac = 0.1, random_state = 120)\n",
    "            test_sample = test.sample(frac = 0.1, random_state=120)\n",
    "            cur_working = os.path.dirname(os.getcwd()) \n",
    "            print(\"Exporting Data To Git Path:\"+ str(cur_working))\n",
    "            if platform == 'darwin':\n",
    "                fptrain = str(cur_working) + \"/dsProject/train_sample.csv\"\n",
    "                fptest = str(cur_working) + \"/dsProject/test_sample.csv\"\n",
    "            else:\n",
    "                fptrain = str(cur_working) + \"\\dsProject\\\\train_sample.csv\"\n",
    "                fptest = str(cur_working) + \"\\dsProject\\\\test_sample.csv\"\n",
    "            print(\"Generating csvs @:\\n\" + fptrain+\"\\n\"+ fptest )\n",
    "            train_sample.to_csv(fptrain)\n",
    "            test_sample.to_csv(fptest)\n",
    "            print(\"Setting Pandas Objects at above csv directories\")\n",
    "            train = pd.read_csv(fptrain)\n",
    "            test = pd.read_csv(fptest)\n",
    "            sys.stdout.write(BLUE)\n",
    "            print(\"\\n Process Complete\")\n",
    "            sys.stdout.write(RESET)\n",
    "            print(\"==============================================================\")\n",
    "        elif error_code == 'n':\n",
    "            print(\"Fatal Error Rerun Chunk\")\n",
    "        else:\n",
    "            sys.exit(0)\n",
    "\n",
    "def set_seed(s=0):\n",
    "    random.seed(s)\n",
    "    os.environ['PYTHONHASHSEED'] = str(s)\n",
    "    np.random.seed(s)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (590540, 433)\n",
      "Test Shape: (506691, 432)\n",
      "NA Varaibles Filled\n",
      "Dropped id cols\n",
      "Mem. usage decreased to 569.17 Mb (68.3% reduction)\n",
      "Mem. usage decreased to 498.38 Mb (67.7% reduction)\n",
      "Intial df's removed from cache\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "\n",
    "set_seed(s=1)\n",
    "\n",
    "train_transaction  = pd.read_csv(\"C:\\\\Users\\\\zaine\\\\OneDrive\\\\Desktop\\\\School\\\\STAT\\\\Reg_Proj\\\\ieee-fraud-detection\\\\train_transaction.csv\", index_col = 'TransactionID')\n",
    "test_transaction = pd.read_csv(\"C:\\\\Users\\\\zaine\\\\OneDrive\\\\Desktop\\\\School\\\\STAT\\\\Reg_Proj\\\\ieee-fraud-detection\\\\test_transaction.csv\", index_col = 'TransactionID')\n",
    "\n",
    "train_identity = pd.read_csv(\"C:\\\\Users\\\\zaine\\\\OneDrive\\\\Desktop\\\\School\\\\STAT\\\\Reg_Proj\\\\ieee-fraud-detection\\\\train_identity.csv\", index_col = 'TransactionID')\n",
    "test_identity = pd.read_csv(\"C:\\\\Users\\\\zaine\\\\OneDrive\\\\Desktop\\\\School\\\\STAT\\\\Reg_Proj\\\\ieee-fraud-detection\\\\test_identity.csv\", index_col = 'TransactionID')\n",
    "\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)\n",
    "\n",
    "\n",
    "y_train = train['isFraud'].copy()\n",
    "y_test = train['isFraud'].copy()\n",
    "\n",
    "#Drop target from train set and fill in Null values\n",
    "F_Train = train.drop('isFraud', axis =1)\n",
    "F_Test  = test.copy()\n",
    "\n",
    "\n",
    "\n",
    "F_Train = F_Train.fillna(-1)\n",
    "F_Test = F_Test.fillna(-1)\n",
    "print(\"NA Varaibles Filled\")\n",
    "\n",
    "F_Train = F_Train.drop(columns=['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'])\n",
    "F_Test = F_Test.drop(columns=['id-01', 'id-02', 'id-03', 'id-04', 'id-05', 'id-06', 'id-07', 'id-08', 'id-09', 'id-10', 'id-11', 'id-12', 'id-13', 'id-14', 'id-15', 'id-16', 'id-17', 'id-18', 'id-19', 'id-20', 'id-21', 'id-22', 'id-23', 'id-24', 'id-25', 'id-26', 'id-27', 'id-28', 'id-29', 'id-30', 'id-31', 'id-32', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38'])\n",
    "\n",
    "\n",
    "print(\"Dropped id cols\")\n",
    "F_Train = reduce_mem_usage(F_Train)\n",
    "F_Test = reduce_mem_usage(F_Test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove those pesky big sets from cache\n",
    "del train_transaction, train_identity, test_transaction, test_identity\n",
    "del train, test\n",
    "print(\"Intial df's removed from cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in F_Train.columns:\n",
    "    if F_Train[i].dtype =='object' or F_Test[i].dtype =='object':\n",
    "        if i in F_Train.columns and i in F_Test.columns:\n",
    "            lab = preprocessing.LabelEncoder()\n",
    "            lab.fit(list(F_Train[i].values) + list(F_Test[i].values))\n",
    "            F_Train[i] = lab.transform(list(F_Train[i].values))\n",
    "            F_Test[i] = lab.transform(list(F_Test[i].values))\n",
    "        else:\n",
    "            sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "gBoostClassifier = xgb.XGBClassifier(\n",
    "n_estimators = 500, \n",
    "max_depth = 9, \n",
    "learning_rate = 0.5,\n",
    "subsample=0.9,\n",
    "colsample_bytree = 0.9,\n",
    "missing = -1,\n",
    "random_state = 1,\n",
    "\n",
    ")\n",
    "gradModel = gBoostClassifier.fit(F_Train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:ProductCD: object, card4: object, card6: object, P_emaildomain: object, R_emaildomain: object, M1: object, M2: object, M3: object, M4: object, M5: object, M6: object, M7: object, M8: object, M9: object, DeviceType: object, DeviceInfo: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-88ea630c69b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgradModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF_Test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zaine\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1523\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m   1524\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1525\u001b[1;33m             class_probs = super().predict(\n\u001b[0m\u001b[0;32m   1526\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m                 \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zaine\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1112\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m                     predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[0;32m   1115\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                         \u001b[0miteration_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zaine\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2281\u001b[0m         \u001b[0menable_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_has_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2283\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_transform_pandas_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2284\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zaine\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[1;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     ):\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[0m_invalid_dataframe_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     feature_names, feature_types = _pandas_feature_info(\n",
      "\u001b[1;32mc:\\Users\\zaine\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_invalid_dataframe_dtype\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DataFrame.dtypes for data must be int, float, bool or category.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:ProductCD: object, card4: object, card6: object, P_emaildomain: object, R_emaildomain: object, M1: object, M2: object, M3: object, M4: object, M5: object, M6: object, M7: object, M8: object, M9: object, DeviceType: object, DeviceInfo: object"
     ]
    }
   ],
   "source": [
    "preds =  gradModel.predict(F_Test)\n",
    "print(preds)\n",
    "\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
