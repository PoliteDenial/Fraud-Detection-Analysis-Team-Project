{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font size= 3, style=\"text-align:justify\">\n",
    "\n",
    "Zain Elsell & Avery Fulton\\\n",
    "March 17, 2023\\\n",
    "Applied Regression \n",
    "\n",
    "<p style=\"text-align: center;\">Fraud Detection Paper 1</p>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As stated in our project proposal our goal is to try to create a logistic regression fraud detection algorithm from the IEEE fraud-detection data set located on Kagel. The primary objectives of this paper are to analyze the response variable in both problem and mathematical contexts, evaluate potential predictor variables, identify and address inconsistencies or outliers, establish confidence intervals for some predictor variables, and determine high correlation coefficients between them.\\\n",
    "   \n",
    "    This is the long verison of that sentence\n",
    "    Primarily the goals of this paper are to identify and explore our response variable both in the context of the problem and mathematically, understand potential \"model features\" or our predictor variables, identify inconsistencies or outliers that may exist within said features, construct confidence intervals for a portion of the aforementioned variables, and finally compute and identify high correlation coffiencents between any of the features.\n",
    "    \n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; That being said, our response variable is the binary predictor \"isFraud\". We can begin this project by first importing the required dependencies that we are going through and then performing some, rudimentary data wrangling. Initially, what we are doing is importing the data for the train and test sets that were given to us then merging the identity data with the transaction data for both our train and test sets on the TransactionID column so we can use only two .csv documents as opposed to the given four. The next step in the wrangling process is taking a consistent sample of 10% of the given data; the primary motivation behind this choice was to reduce the compute time for a select few and manipulations operations, as the initial data set contains 590,540 data points and 434 possible features. Note that we will run the created model on the overall data set to consistently represent the performance in the context of a larger sample. \n",
    "    \n",
    "\n",
    "    \n",
    " </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "data_path = \"~/Desktop/School/Reg_Proj/ieee-fraud-detection/\" #\n",
    "\n",
    "train_ident = pd.read_csv(data_path + 'train_identity.csv')\n",
    "test_ident = pd.read_csv(data_path + 'test_identity.csv')\n",
    "train_transaction = pd.read_csv(data_path + 'train_transaction.csv')\n",
    "test_transaction = pd.read_csv(data_path + 'test_transaction.csv')\n",
    "\n",
    "#Merging our 4 data sets into 2:\n",
    "train = train_transaction.merge(train_ident, on='TransactionID', how='left')\n",
    "test = test_transaction.merge(test_ident, on='TransactionID', how='left')\n",
    "# Sampling 10% of the data to run our analysis on \n",
    "#Seed of sample is 120  we should use theat for the rest  of the proj\n",
    "train_sample = train.sample(frac = 0.1, random_state = 120)\n",
    "test_sample = test.sample(frac = 0.1, random_state=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the proportion of transactions that are fraud for our smaller data set:\n",
    "propFraud10 = train_sample['isFraud'].mean()\n",
    "print(propFraud10)\n",
    "\n",
    "\n",
    "#for the larger data set\n",
    "propFraud = train['isFraud'].mean()\n",
    "print(propFraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size= 3, style=\"text-align:justify\">\n",
    "    \n",
    "Since our response varaible is binary it deosn't make the most, sense to graph it in terms of a histogram however we can say.  \n",
    "\n",
    " \n",
    "$$\\text{Let } F := \\text{the probability that a transaction is fraudelent}$$\n",
    "\n",
    "$$\\text{Let }F':= \\text{the probability that a transaction is fraudelent when sampling 10 % of the data}$$ \n",
    "\n",
    "$$F \\sim\\text{Binomial(3.499%)}$$\n",
    "\n",
    "$$F'\\sim\\text{Binomial(3.483%)}$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generally we can say our response variable is distributed as binomial with a $p \\approx 3.5$, and some of the clear implications of finding is that likelyhood of fraud is not common outcome of a transaction. Reflecting from the the finicial sector context this is plausible the overall proporition of fraudelent transactions, is signicantly lower then the proporition authorized ones. \n",
    " <\\font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_corr = train_sample.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('C1', 'C2'): 0.9960255814335263,\n",
       " ('C1', 'C4'): 0.9720331304828849,\n",
       " ('C1', 'C6'): 0.9837587217717109,\n",
       " ('C1', 'C7'): 0.9379304598092051,\n",
       " ('C1', 'C8'): 0.9726319845008362,\n",
       " ('C1', 'C10'): 0.9649521721724863,\n",
       " ('C1', 'C11'): 0.9970126813477574,\n",
       " ('C1', 'C12'): 0.9393597650111772,\n",
       " ('C1', 'C13'): 0.7926063809278694,\n",
       " ('C1', 'C14'): 0.9559284351187977}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the correlations as a DataFrame\n",
    "abscorr = train_corr.abs()\n",
    "# Filter the DataFrame to show only correlations greater than 0.7 and not equal to 1\n",
    "high_corr = abscorr[(abscorr > 0.7) & (abscorr != 1)]\n",
    "# Print the high correlation pairs\n",
    "high_corr_dict = (high_corr.stack().drop_duplicates()).to_dict()\n",
    "#high_corr_dict.keys()\n",
    "corrs = {k:v for k, v in high_corr_dict.items() if 'C1' in k}\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034967995394046124\n",
      "0.03499000914417313\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
