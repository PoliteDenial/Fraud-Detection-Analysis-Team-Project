{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size= 3, style=\"text-align:justify\">\n",
    "\n",
    "Zain Elsell & Avery Fulton\\\n",
    "March 17, 2023\\\n",
    "Applied Regression \n",
    "\n",
    "<p style=\"text-align: center;\">Fraud Detection Paper 1</p>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As stated in our project proposal our goal is to try to create a logistic regression fraud detection algorithm from the IEEE fraud-detection data set located on Kagel. The primary objectives of this paper are to analyze the response variable in both problem and mathematical contexts, evaluate potential predictor variables, identify and address inconsistencies or outliers, establish confidence intervals for some predictor variables, and determine high correlation coefficients between them.    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We can begin this project by first importing the required dependencies that we are going through and then performing some, rudimentary data wrangling. Initially, what we are doing is importing the data for the train and test sets that were given to us then merging the identity data with the transaction data for both our train and test sets on the TransactionID column so we can use only two .csv documents as opposed to the given four. The next step in the wrangling process is taking a consistent sample of 10% of the given data; the primary motivation behind this choice was to reduce the compute time for a select few and manipulations operations, as the initial data set contains 590,540 data points and 434 possible features. Note that we will run the created model on the overall data set to consistently represent the performance in the context of a larger sample. \n",
    "    \n",
    "\n",
    "    \n",
    " </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axis import Axis\n",
    "import scipy.stats as sci\n",
    "import sys, os\n",
    "from sys import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all of the stuff this doing is litearlly just making everything cross compatiable depending on who and what system they are working on along wiht data wrangling\n",
    "\n",
    "RED   = \"\\033[1;31m\"  \n",
    "BLUE  = \"\\033[1;34m\"\n",
    "RESET = \"\\033[0;0m\"\n",
    "BOLD    = \"\\033[;1m\"\n",
    "REVERSE = \"\\033[;7m\"\n",
    "\n",
    "np.random.seed(120)\n",
    "\n",
    "#setting path  \n",
    "if platform == \"win32\" and os.getlogin() == 'zaine':\n",
    "    data_path = 'C:/Users/zaine/OneDrive/Desktop/School/STAT/Reg_Proj/ieee-fraud-detection/'\n",
    "    ScaleData = 0\n",
    "elif platform == \"win32\" and os.getlogin() == 'Avery':\n",
    "    data_path = \"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/\"\n",
    "    ScaleData = 1\n",
    "elif platform == \"darwin\":\n",
    "    data_path = \"/Users/zain/Desktop/School/Reg_Proj/ieee-fraud-detection/\"\n",
    "    ScaleData = 2\n",
    "    \n",
    "\n",
    "#a bunch of logic related to what system we are on\n",
    "if(ScaleData == 0):\n",
    "    train_ident = pd.read_csv(data_path + 'train_identity.csv')\n",
    "    test_ident = pd.read_csv(data_path + 'test_identity.csv')\n",
    "    train_transaction = pd.read_csv(data_path + 'train_transaction.csv')\n",
    "    test_transaction = pd.read_csv(data_path + 'test_transaction.csv')\n",
    "    #Merging our 4 data sets into 2:\n",
    "    train = train_transaction.merge(train_ident, on='TransactionID', how='left')\n",
    "    test = test_transaction.merge(test_ident, on='TransactionID', how='left')\n",
    "    \n",
    "    cur_working = os.path.dirname(os.getcwd()) \n",
    "    fptrain = str(cur_working) + \"\\dsProject\\\\train_sample.csv\"\n",
    "    fptest = str(cur_working) + \"\\dsProject\\\\test_sample.csv\"\n",
    "\n",
    "    train_sample = pd.read_csv(fptrain)\n",
    "    test_sample = pd.read_csv(fptest)\n",
    "\n",
    "    \n",
    "elif(ScaleData == 1):\n",
    "    train = pd.read_csv(data_path + \"/train_sample.csv\") #make this exact path to the csv\n",
    "    test = pd.read_csv(data_path + \"/test_sample.csv\") #make this exact path to the csv\n",
    "    train_sample = pd.read_csv(data_path + \"/train_sample.csv\") #just to prevent edge cases later \n",
    "    test_sample = pd.read_csv(data_path + \"/train_sample.csv\")\n",
    "elif(ScaleData == 2):\n",
    "    train = pd.read_csv(data_path + \"/train_sample.csv\")\n",
    "    test = pd.read_csv(data_path + \"/test_sample.csv\")\n",
    "    train_sample = pd.read_csv(data_path + \"/train_sample.csv\")\n",
    "    test_sample = pd.read_csv(data_path + \"/train_sample.csv\")\n",
    "else:\n",
    "    sys.stdout.write(RED)\n",
    "    error_code= input(\"Error: Data path not found do you want to regenearte the train and test sets in the current working directory?[y/n]: \")\n",
    "    if error_code == 'y':\n",
    "        print(\"Warning: Data path not found regenerating data in current script working directory!\")\n",
    "        sys.stdout.write(RESET)\n",
    "        print(\"\\n==============================================================\")\n",
    "        train_ident = pd.read_csv(data_path + 'train_identity.csv')\n",
    "        test_ident = pd.read_csv(data_path + 'test_identity.csv')\n",
    "        train_transaction = pd.read_csv(data_path + 'train_transaction.csv')\n",
    "        test_transaction = pd.read_csv(data_path + 'test_transaction.csv')\n",
    "        #Merging our 4 data sets into 2:\n",
    "        print(\"Merging Data\")\n",
    "        train = train_transaction.merge(train_ident, on='TransactionID', how='left')\n",
    "        test = test_transaction.merge(test_ident, on='TransactionID', how='left')\n",
    "        print(\"Sampling Data\")\n",
    "        train_sample = train.sample(frac = 0.1, random_state = 120)\n",
    "        test_sample = test.sample(frac = 0.1, random_state=120)\n",
    "        cur_working = os.path.dirname(os.getcwd()) \n",
    "        print(\"Exporting Data To Git Path:\"+ str(cur_working))\n",
    "        if platform == 'darwin':\n",
    "            fptrain = str(cur_working) + \"/dsProject/train_sample.csv\"\n",
    "            fptest = str(cur_working) + \"/dsProject/test_sample.csv\"\n",
    "        else:\n",
    "            fptrain = str(cur_working) + \"\\dsProject\\\\train_sample.csv\"\n",
    "            fptest = str(cur_working) + \"\\dsProject\\\\test_sample.csv\"\n",
    "        print(\"Generating csvs @:\\n\" + fptrain+\"\\n\"+ fptest )\n",
    "        train_sample.to_csv(fptrain)\n",
    "        test_sample.to_csv(fptest)\n",
    "        print(\"Setting Pandas Objects at above csv directories\")\n",
    "        train = pd.read_csv(fptrain)\n",
    "        test = pd.read_csv(fptest)\n",
    "        sys.stdout.write(BLUE)\n",
    "        print(\"\\n Process Complete\")\n",
    "        sys.stdout.write(RESET)\n",
    "        print(\"==============================================================\")\n",
    "    elif error_code == 'n':\n",
    "        print(\"Fatal Error Rerun Chunk\")\n",
    "    else:\n",
    "        sys.exit(0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propFraud = train['isFraud'].mean()\n",
    "print(propFraud)\n",
    "\n",
    "#constructing a sampling distribution.\n",
    "p = propFraud\n",
    "n = 1000 \n",
    "#gen 1k samples of size 1000\n",
    "samples = np.random.binomial(n,p, 1000)\n",
    "sample_props = samples/n\n",
    "\n",
    "plt.hist(sample_props, bins=50, density=True, alpha=0.5, label='Sampling Distribution')\n",
    "mu = np.mean(sample_props)\n",
    "print(mu)\n",
    "var = np.var(sample_props)\n",
    "print(var)\n",
    "sigma = np.std(sample_props)\n",
    "\n",
    "x = np.linspace(0, 0.28, 1000)\n",
    "norm = sci.norm.pdf(x, loc=mu, scale=sigma)\n",
    "plt.axis([0, .06, 0, 120])\n",
    "plt.axvline(x=mu, color='r')\n",
    "plt.plot(x, norm, label='Normal Distribution')\n",
    "plt.xlabel('Proportion')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size= 3, style=\"text-align:justify\">\n",
    "     \n",
    "$$\\text{Let } F := \\text{A transaction is fraudelent}$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Our response varaible is the value of the \"isFraud\" within the data set, in order to find the distribution of our response varaible we first calculated the proportion of fraudelent transactions to non fraudlent ones which a approximate value of 3.499%, we then construct a sampling distribution using a binomial random variable using the previously calculated proportion, and then finally ploted the randomly generated samples and compare it to a normal distribution centered at the mean of sampled proporitions, with standard deviation. Comparing the simulated values to the normal distribution then gives the final result that, $F \\sim\\text{Normal}(\\mu = 0.35213, \\sigma^2 = 3.4923631 \\cdot 10^{-5})$ (using the large data set) by the central limit theorm. Some of the clear implications of finding is that likelyhood of fraud is not common outcome of a transaction. Reflecting from the the finicial sector context this is plausible the overall proporition of fraudelent transactions, is signicantly lower then the proporition authorized ones. \n",
    " \n",
    " </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = train_sample.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the correlations as a DataFrame\n",
    "abscorr = train_corr.abs()\n",
    "# Filter the DataFrame to show only correlations greater than 0.7 and not equal to 1\n",
    "high_corr = abscorr[(abscorr > 0.7) & (abscorr != 1)]\n",
    "# Print the high correlation pairs\n",
    "high_corr_dict = (high_corr.stack().drop_duplicates()).to_dict()\n",
    "#high_corr_dict.keys()\n",
    "corrs = {k:v for k, v in high_corr_dict.items() if 'C1' in k}\n",
    "corrs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size= 3, style=\"text-align:justify\">\n",
    "     \n",
    "$$\\text{Let } F := \\text{the probability that a transaction is fraudelent}$$\n",
    "\n",
    "$$\\text{Let }F':= \\text{the probability that a transaction is fraudelent when sampling 10 \\% of the data}$$ \n",
    "\n",
    "$$F \\sim\\text{Binomial(3.499\\%)}$$\n",
    "\n",
    "$$F'\\sim\\text{Binomial(3.483\\%)}$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generally we can say our response variable is distributed as binomial with a $p \\approx 3.5$, and some of the clear implications of finding is that likelyhood of fraud is not common outcome of a transaction. Reflecting from the the finicial sector context this is plausible the overall proporition of fraudelent transactions, is signicantly lower then the proporition authorized ones. \n",
    " \n",
    " </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
