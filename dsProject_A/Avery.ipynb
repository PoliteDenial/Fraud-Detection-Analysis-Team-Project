{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import scipy.stats as sci\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/test_sample.csv\")\n",
    "train = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/train_sample.csv\")\n",
    "train.drop(columns='Unnamed: 0',inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the columns with T/F and making them 1/0\n",
    "for i in range(1,10):\n",
    "    train['M'+str(i)] = train['M'+str(i)].replace({'T': 1, 'F': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code I found that will go through and mark the unique entries in each column, giving the general estimate of what each does\n",
    "for col, values in train.iteritems():\n",
    "    num_uniques = values.nunique()\n",
    "    print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "    print (values.unique())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypething = {k:v for k, v in (train.dtypes == 'O').to_dict().items() if v is True}\n",
    "\n",
    "\n",
    "for k, v in dtypething.items():\n",
    "    train[k] = train[k].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Distribution Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = train[train['TransactionAmt'] >0 ]['TransactionAmt']\n",
    "\n",
    "# Dist of TransactionAmt\n",
    "fig = px.histogram(\n",
    "    transactions, \n",
    "    title= \"Distribuiton of Transactions\",\n",
    "    nbins=1000\n",
    "    ) \n",
    "\n",
    "fig.update_layout( height=500, width=1000)\n",
    "fig.update_xaxes(range = [0,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = train[train['TransactionAmt'] > 0 ]['TransactionAmt']\n",
    "\n",
    "# Dist of TransactionAmt\n",
    "fig = px.box(\n",
    "    transactions, \n",
    "    title= \"Distribuiton of Transactions\"\n",
    "    ) \n",
    "\n",
    "fig.update_layout( height=500, width=750)\n",
    "# fig.update_xaxes(range = [0,5000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Card_Dist = pd.value_counts(train['card4'],normalize= True, dropna = False)\n",
    "\n",
    "# Dist of Card Types\n",
    "fig = px.bar( \n",
    "    data_frame = Card_Dist,\n",
    "    title='Distribution of Card Type',color = Card_Dist.keys(),\n",
    "    text= Card_Dist.apply(lambda x: f'{x*100:.2f}%'),\n",
    "    orientation='h'\n",
    "    )\n",
    "\n",
    "fig.update_layout(xaxis={'title': 'Card Types'},yaxis={'title': 'Percentage'}, height=500, width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.value_counts(train['DeviceInfo'],normalize= True, dropna = False)\n",
    "\n",
    "amount_to_show = 9\n",
    "new_data = {}\n",
    "\n",
    "for i, (key, value) in enumerate(dist.to_dict().items()):\n",
    "    if i < amount_to_show:\n",
    "        new_data[key] = value\n",
    "    else:\n",
    "        new_data['Other'] = new_data.get('Other', 0) + value\n",
    "\n",
    "fig = px.pie(\n",
    "    values=list(new_data.values()), \n",
    "    names=list(new_data.keys()),\n",
    "    title= 'Distribution Of Operating System That Made The Transaction'\n",
    "    )\n",
    "\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Confidence Interval Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proplist = [1,2,3,5,6,7,8,9]\n",
    "card_props = []\n",
    "for i in proplist:\n",
    "    prop = train['M'+str(i)].mean()\n",
    "    card_props.append(prop)\n",
    "total_card = len(card_props)\n",
    "\n",
    "for i in card_props:\n",
    "    se = np.sqrt(i*(1-i)/total_card)\n",
    "    me = sci.t.ppf(0.975, total_card-1)*se\n",
    "    ci = (i-me, i+me)\n",
    "    print('=======================================')\n",
    "    print('Sample Proportion:', i)\n",
    "    print('Confidence Interval (95%):', ci)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Some Random PCA stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supertestset = train.copy()\n",
    "\n",
    "supertestset = pd.get_dummies(supertestset,prefix_sep = '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some PCA testing\n",
    "\n",
    "\n",
    "\n",
    "supertestset2 = supertestset.copy()\n",
    "supertestset2.dropna(axis= 1,inplace= True)\n",
    "\n",
    "# data_scalar = StandardScaler()\n",
    "# data_scalar.fit(supertestset2)\n",
    "# scaled_data_frame = data_scalar.transform(supertestset2)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(scaled_data_frame)\n",
    "# x_pca = pca.transform(scaled_data_frame)\n",
    "\n",
    "\n",
    "# plt.scatter(x_pca[:,0],x_pca[:,1],c=supertestset2['isFraud'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Random Forest Stuff\n",
    "\n",
    "Our top 10 vars!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = supertestset2.drop(['isFraud'],axis = 1)\n",
    "Y = supertestset2['isFraud']\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "X2 = StandardScaler().fit_transform(X)\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X2,Y,test_size = 0.3, random_state = 101)\n",
    "\n",
    "trainedForest = RandomForestClassifier(n_estimators = 700).fit(X_Train,Y_Train)\n",
    "prediciton = trainedForest.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(trainedForest.feature_importances_, index = X.columns)\n",
    "feat_importances.nlargest(7).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = feat_importances.nlargest(10).keys().to_list()\n",
    "\n",
    "X_Reduced = X[importance]\n",
    "X_Reduced = StandardScaler().fit_transform(X_Reduced)\n",
    "X_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(X_Reduced, Y, test_size = 0.30, random_state = 101)\n",
    "\n",
    "\n",
    "trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train2,Y_Train2)\n",
    "predictionforest = trainedforest.predict(X_Test2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERY SLIGHT DROP IN PREFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('================================================================')\n",
    "print('All Features')\n",
    "print(classification_report(Y_Test,prediciton))\n",
    "print('================================================================')\n",
    "print('Top 10 Features')\n",
    "print(classification_report(Y_Test2,predictionforest))\n",
    "print('================================================================')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cbd6a0ecf300be7ba263fe2db5340765cbb28739d1507e0fb990b57418d6190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
