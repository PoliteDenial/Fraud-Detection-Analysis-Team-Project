{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import scipy.stats as sci\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/test_sample.csv\")\n",
    "train = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/train_sample.csv\")\n",
    "train.drop(columns='Unnamed: 0',inplace= True)\n",
    "test.drop(columns='Unnamed: 0',inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avery\\Documents\\GitHub\\Regression_proj\\dsProject_A\\Avery.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Avery/Documents/GitHub/Regression_proj/dsProject_A/Avery.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m sfm \u001b[39m=\u001b[39m SelectFromModel(rf, max_features\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Avery/Documents/GitHub/Regression_proj/dsProject_A/Avery.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# fit SFM on train dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Avery/Documents/GitHub/Regression_proj/dsProject_A/Avery.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m sfm\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Avery/Documents/GitHub/Regression_proj/dsProject_A/Avery.ipynb#X43sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# transform train and test datasets with selected features\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Avery/Documents/GitHub/Regression_proj/dsProject_A/Avery.ipynb#X43sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m X_train_selected \u001b[39m=\u001b[39m sfm\u001b[39m.\u001b[39mtransform(X_train)\n",
      "File \u001b[1;32mc:\\Users\\Avery\\Anaconda3\\envs\\minimal_ds\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:351\u001b[0m, in \u001b[0;36mSelectFromModel.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_ \u001b[39m=\u001b[39m clone(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator)\n\u001b[1;32m--> 351\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_, \u001b[39m\"\u001b[39m\u001b[39mfeature_names_in_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names_in_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_\u001b[39m.\u001b[39mfeature_names_in_\n",
      "File \u001b[1;32mc:\\Users\\Avery\\Anaconda3\\envs\\minimal_ds\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:331\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 331\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    332\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[0;32m    333\u001b[0m )\n\u001b[0;32m    334\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\Avery\\Anaconda3\\envs\\minimal_ds\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Avery\\Anaconda3\\envs\\minimal_ds\\lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[1;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Avery\\Anaconda3\\envs\\minimal_ds\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[0;32m    900\u001b[0m             array,\n\u001b[0;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    904\u001b[0m         )\n\u001b[0;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\Avery\\Anaconda3\\envs\\minimal_ds\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)\n",
    "\n",
    "\n",
    "# separate predictor column from train dataset\n",
    "X_train = train.drop(columns=['isFraud'])\n",
    "y_train = train['isFraud']\n",
    "\n",
    "# load test dataset\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "# create an instance of RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# create a SelectFromModel transformer with RF as the base estimator\n",
    "sfm = SelectFromModel(rf, max_features=50)\n",
    "\n",
    "# fit SFM on train dataset\n",
    "sfm.fit(X_train, y_train)\n",
    "\n",
    "# transform train and test datasets with selected features\n",
    "X_train_selected = sfm.transform(X_train)\n",
    "X_test_selected = sfm.transform(test_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the columns with T/F and making them 1/0\n",
    "for i in range(1,10):\n",
    "    train['M'+str(i)] = train['M'+str(i)].replace({'T': 1, 'F': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code I found that will go through and mark the unique entries in each column, giving the general estimate of what each does\n",
    "for col, values in train.iteritems():\n",
    "    num_uniques = values.nunique()\n",
    "    print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "    print (values.unique())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypething = {k:v for k, v in (train.dtypes == 'O').to_dict().items() if v is True}\n",
    "\n",
    "\n",
    "for k, v in dtypething.items():\n",
    "    train[k] = train[k].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Distribution Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = train[train['TransactionAmt'] >0 ]['TransactionAmt']\n",
    "\n",
    "# Dist of TransactionAmt\n",
    "fig = px.histogram(\n",
    "    transactions, \n",
    "    title= \"Distribuiton of Transactions\",\n",
    "    nbins=1000\n",
    "    ) \n",
    "\n",
    "fig.update_layout( height=500, width=1000)\n",
    "fig.update_xaxes(range = [0,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = train[train['TransactionAmt'] > 0 ]['TransactionAmt']\n",
    "\n",
    "# Dist of TransactionAmt\n",
    "fig = px.box(\n",
    "    transactions, \n",
    "    title= \"Distribuiton of Transactions\"\n",
    "    ) \n",
    "\n",
    "fig.update_layout( height=500, width=750)\n",
    "# fig.update_xaxes(range = [0,5000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Card_Dist = pd.value_counts(train['card4'],normalize= True, dropna = False)\n",
    "\n",
    "# Dist of Card Types\n",
    "fig = px.bar( \n",
    "    data_frame = Card_Dist,\n",
    "    title='Distribution of Card Type',color = Card_Dist.keys(),\n",
    "    text= Card_Dist.apply(lambda x: f'{x*100:.2f}%'),\n",
    "    orientation='h'\n",
    "    )\n",
    "\n",
    "fig.update_layout(xaxis={'title': 'Card Types'},yaxis={'title': 'Percentage'}, height=500, width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.value_counts(train['DeviceInfo'],normalize= True, dropna = False)\n",
    "\n",
    "amount_to_show = 9\n",
    "new_data = {}\n",
    "\n",
    "for i, (key, value) in enumerate(dist.to_dict().items()):\n",
    "    if i < amount_to_show:\n",
    "        new_data[key] = value\n",
    "    else:\n",
    "        new_data['Other'] = new_data.get('Other', 0) + value\n",
    "\n",
    "fig = px.pie(\n",
    "    values=list(new_data.values()), \n",
    "    names=list(new_data.keys()),\n",
    "    title= 'Distribution Of Operating System That Made The Transaction'\n",
    "    )\n",
    "\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Confidence Interval Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proplist = [1,2,3,5,6,7,8,9]\n",
    "card_props = []\n",
    "for i in proplist:\n",
    "    prop = train['M'+str(i)].mean()\n",
    "    card_props.append(prop)\n",
    "total_card = len(card_props)\n",
    "\n",
    "for i in card_props:\n",
    "    se = np.sqrt(i*(1-i)/total_card)\n",
    "    me = sci.t.ppf(0.975, total_card-1)*se\n",
    "    ci = (i-me, i+me)\n",
    "    print('=======================================')\n",
    "    print('Sample Proportion:', i)\n",
    "    print('Confidence Interval (95%):', ci)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Some Random PCA stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supertestset = train.copy()\n",
    "\n",
    "supertestset = pd.get_dummies(supertestset,prefix_sep = '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some PCA testing\n",
    "\n",
    "\n",
    "\n",
    "supertestset2 = supertestset.copy()\n",
    "supertestset2.dropna(axis= 1,inplace= True)\n",
    "\n",
    "# data_scalar = StandardScaler()\n",
    "# data_scalar.fit(supertestset2)\n",
    "# scaled_data_frame = data_scalar.transform(supertestset2)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(scaled_data_frame)\n",
    "# x_pca = pca.transform(scaled_data_frame)\n",
    "\n",
    "\n",
    "# plt.scatter(x_pca[:,0],x_pca[:,1],c=supertestset2['isFraud'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Random Forest Stuff\n",
    "\n",
    "Our top 10 vars!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = supertestset2.drop(['isFraud'],axis = 1)\n",
    "Y = supertestset2['isFraud']\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "X2 = StandardScaler().fit_transform(X)\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X2,Y,test_size = 0.3, random_state = 101)\n",
    "\n",
    "# trainedForest = RandomForestClassifier(n_estimators = 700).fit(X_Train,Y_Train)\n",
    "# prediciton = trainedForest.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(trainedForest.feature_importances_, index = X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = feat_importances.nlargest(10).keys().to_list()\n",
    "\n",
    "X_Reduced = X[importance]\n",
    "X_Reduced = StandardScaler().fit_transform(X_Reduced)\n",
    "X_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(X_Reduced, Y, test_size = 0.30, random_state = 101)\n",
    "\n",
    "\n",
    "trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train2,Y_Train2)\n",
    "predictionforest = trainedforest.predict(X_Test2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERY SLIGHT DROP IN PREFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('================================================================')\n",
    "print('All Features')\n",
    "print(classification_report(Y_Test,prediciton))\n",
    "print('================================================================')\n",
    "print('Top 10 Features')\n",
    "print(classification_report(Y_Test2,predictionforest))\n",
    "print('================================================================')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_data = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/test_sample.csv\")\n",
    "train_data = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/train_sample.csv\")\n",
    "\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "train_data = pd.get_dummies(train_data)\n",
    "test_data = pd.get_dummies(test_data)\n",
    "train_data.fillna(0)\n",
    "test_data.fillna(0)\n",
    "\n",
    "# Defining the predictor and target variables for training and test data\n",
    "X_train = train_data.drop('isFraud', axis=1)\n",
    "y_train = train_data['isFraud']\n",
    "X_test = test_data\n",
    "\n",
    "# Creating a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fitting the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the target variable for the testing data\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns = X_test.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cbd6a0ecf300be7ba263fe2db5340765cbb28739d1507e0fb990b57418d6190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
