{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import scipy.stats as sci\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/test_sample.csv\")\n",
    "train = pd.read_csv(\"C:/Users/Avery/Documents/GitHub/Regression_proj/dsProject/train_sample.csv\")\n",
    "\n",
    "# test = pd.get_dummies(test)\n",
    "# train = pd.get_dummies(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "\n",
    "# train = pd.get_dummies(train)\n",
    "# test = pd.get_dummies(test)\n",
    "\n",
    "\n",
    "# # separate predictor column from train dataset\n",
    "# X_train = train.drop(columns=['isFraud'])\n",
    "# y_train = train['isFraud']\n",
    "\n",
    "# # load test dataset\n",
    "# # test_df = pd.read_csv(\"test.csv\")\n",
    "# # create an instance of RandomForestClassifier\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# # create a SelectFromModel transformer with RF as the base estimator\n",
    "# sfm = SelectFromModel(rf, max_features=50)\n",
    "\n",
    "# # fit SFM on train dataset\n",
    "# sfm.fit(X_train, y_train)\n",
    "\n",
    "# # transform train and test datasets with selected features\n",
    "# X_train_selected = sfm.transform(X_train)\n",
    "# X_test_selected = sfm.transform(test_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the columns with T/F and making them 1/0\n",
    "for i in range(1,10):\n",
    "    train['M'+str(i)] = train['M'+str(i)].replace({'T': 1, 'F': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code I found that will go through and mark the unique entries in each column, giving the general estimate of what each does\n",
    "for col, values in train.iteritems():\n",
    "    num_uniques = values.nunique()\n",
    "    print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "    print (values.unique())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypething = {k:v for k, v in (train.dtypes == 'O').to_dict().items() if v is True}\n",
    "\n",
    "\n",
    "for k, v in dtypething.items():\n",
    "    train[k] = train[k].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Distribution Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = train[train['TransactionAmt'] >0 ]['TransactionAmt']\n",
    "\n",
    "# Dist of TransactionAmt\n",
    "fig = px.histogram(\n",
    "    transactions, \n",
    "    title= \"Distribuiton of Transactions\",\n",
    "    nbins=1000\n",
    "    ) \n",
    "\n",
    "fig.update_layout( height=500, width=1000)\n",
    "fig.update_xaxes(range = [0,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = train[train['TransactionAmt'] > 0 ]['TransactionAmt']\n",
    "\n",
    "# Dist of TransactionAmt\n",
    "fig = px.box(\n",
    "    transactions, \n",
    "    title= \"Distribuiton of Transactions\"\n",
    "    ) \n",
    "\n",
    "fig.update_layout( height=500, width=750)\n",
    "# fig.update_xaxes(range = [0,5000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Card_Dist = pd.value_counts(train['card4'],normalize= True, dropna = False)\n",
    "\n",
    "# Dist of Card Types\n",
    "fig = px.bar( \n",
    "    data_frame = Card_Dist,\n",
    "    title='Distribution of Card Type',color = Card_Dist.keys(),\n",
    "    text= Card_Dist.apply(lambda x: f'{x*100:.2f}%'),\n",
    "    orientation='h'\n",
    "    )\n",
    "\n",
    "fig.update_layout(xaxis={'title': 'Card Types'},yaxis={'title': 'Percentage'}, height=500, width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.value_counts(train['DeviceInfo'],normalize= True, dropna = False)\n",
    "\n",
    "amount_to_show = 9\n",
    "new_data = {}\n",
    "\n",
    "for i, (key, value) in enumerate(dist.to_dict().items()):\n",
    "    if i < amount_to_show:\n",
    "        new_data[key] = value\n",
    "    else:\n",
    "        new_data['Other'] = new_data.get('Other', 0) + value\n",
    "\n",
    "fig = px.pie(\n",
    "    values=list(new_data.values()), \n",
    "    names=list(new_data.keys()),\n",
    "    title= 'Distribution Of Operating System That Made The Transaction'\n",
    "    )\n",
    "\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Correlation Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract the correlations as a DataFrame\n",
    "abscorr = train_corr.abs()\n",
    "# Filter the DataFrame to show only correlations greater than 0.7 and not equal to 1\n",
    "high_corr = abscorr[(abscorr > 0) & (abscorr != 1)]\n",
    "# Print the high correlation pairs\n",
    "high_corr_dict = (high_corr.stack().drop_duplicates()).to_dict()\n",
    "\n",
    "# Filter the high correlation pairs by 'isFraud' and any column that starts with 'C'\n",
    "corrs = {k:v for k, v in high_corr_dict.items() if ('isFraud' in k) and any(col.startswith('C') for col in k)}\n",
    "\n",
    "# Sort the pairs by correlation value in descending order\n",
    "sorted_corrs = sorted(corrs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the column with the highest correlation with 'isFraud'\n",
    "highest_corr_column = sorted_corrs[0][0][1:]\n",
    "\n",
    "print(\"The column with the highest correlation with 'isFraud' is:\", highest_corr_column)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Confidence Interval Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proplist = [1,2,3,5,6,7,8,9]\n",
    "card_props = []\n",
    "for i in proplist:\n",
    "    prop = train['M'+str(i)].mean()\n",
    "    card_props.append(prop)\n",
    "total_card = len(card_props)\n",
    "\n",
    "for i in card_props:\n",
    "    se = np.sqrt(i*(1-i)/total_card)\n",
    "    me = sci.t.ppf(0.975, total_card-1)*se\n",
    "    ci = (i-me, i+me)\n",
    "    print('=======================================')\n",
    "    print('Sample Proportion:', i)\n",
    "    print('Confidence Interval (95%):', ci)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Random Forest Stuff\n",
    "\n",
    "Our top 10 vars!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "supertrainset = train.copy()\n",
    "# supertrainset.drop(columns=['C1','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13'], inplace=True)\n",
    "\n",
    "yass = supertrainset.reset_index(drop=True)  # update here\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "supertrainset = pd.DataFrame(imputer.fit_transform(yass), columns=yass.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = supertrainset.drop(['isFraud'],axis = 1)\n",
    "Y = supertrainset['isFraud']\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "X2 = StandardScaler().fit_transform(X)\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X2,Y,test_size = 0.3, random_state = 101)\n",
    "\n",
    "trainedForest = RandomForestClassifier(n_estimators = 700).fit(X_Train,Y_Train)\n",
    "prediciton = trainedForest.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(trainedForest.feature_importances_, index = X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = feat_importances.nlargest(10).keys().to_list()\n",
    "\n",
    "X_Reduced = X[importance]\n",
    "X_Reduced = StandardScaler().fit_transform(X_Reduced)\n",
    "X_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(X_Reduced, Y, test_size = 0.30, random_state = 101)\n",
    "\n",
    "\n",
    "trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train2,Y_Train2)\n",
    "predictionforest = trainedforest.predict(X_Test2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERY SLIGHT DROP IN PREFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('================================================================')\n",
    "print('All Features')\n",
    "print(classification_report(Y_Test,prediciton))\n",
    "print('================================================================')\n",
    "print('Top 10 Features')\n",
    "print(classification_report(Y_Test2,predictionforest))\n",
    "print('================================================================')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cbd6a0ecf300be7ba263fe2db5340765cbb28739d1507e0fb990b57418d6190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
